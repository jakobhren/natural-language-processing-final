{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing Final Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real - Fake News Classification: A Comparison of Natural Language Models for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.base import clone\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "\n",
    "df_raw = pd.read_csv(\"data/news_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                                text subject  \\\n",
       "0  Donald Trump just couldn t wish all Americans ...    News   \n",
       "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
       "2  On Friday, it was revealed that former Milwauk...    News   \n",
       "3  On Christmas day, Donald Trump announced that ...    News   \n",
       "4  Pope Francis used his annual Christmas Day mes...    News   \n",
       "\n",
       "                date  label  \n",
       "0  December 31, 2017      0  \n",
       "1  December 31, 2017      0  \n",
       "2  December 30, 2017      0  \n",
       "3  December 29, 2017      0  \n",
       "4  December 25, 2017      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jakobhren/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jakobhren/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jakobhren/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \n",
    "    text = text.str.lower()\n",
    "    \n",
    "    # this remove links and twitter handels, and emails etc\n",
    "    text = text.str.replace(r'http\\S+|www\\S+|https\\S+', '', regex=True)\n",
    "    text = text.str.replace(r'\\S+@\\S+', '', regex=True)\n",
    "    text = text.str.replace(r'@\\w+', '', regex=True)\n",
    "    \n",
    "    text = text.str.replace(f'[{string.punctuation}]', '', regex=True)\n",
    "    \n",
    "    text = text.str.replace(r'\\d+', '', regex=True)\n",
    "    \n",
    "    text = text.str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "    \n",
    "    text_tokens = text.apply(nltk.word_tokenize)\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text_tokens = text_tokens.apply(lambda tokens: [w for w in tokens if w not in stop_words])\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text_tokens = text_tokens.apply(lambda tokens: [lemmatizer.lemmatize(w) for w in tokens])\n",
    "    \n",
    "    processed_text = text_tokens.apply(lambda tokens: ' '.join(tokens))\n",
    "    \n",
    "    return processed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"title_preprocessed\"] = preprocess_text(df[\"title\"])\n",
    "df[\"text_preprocessed\"] = preprocess_text(df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"features\"] = df[\"title_preprocessed\"] + \" \" + df[\"text_preprocessed\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train - Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"features\"]\n",
    "y = df[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.30, stratify=y, random_state=7)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.50, stratify=y_temp, random_state=7)\n",
    "\n",
    "\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_val = X_val.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_val = y_val.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27110,)\n",
      "(5809,)\n",
      "(5810,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions for efficient model evaluation and fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(y_true, y_pred_dict, label=None):\n",
    "    \"\"\"\n",
    "    Evaluate and compare multiple models' predictions.\n",
    "    Parameters:\n",
    "    y_true : array-like\n",
    "        True labels.\n",
    "    y_pred_dict : dict\n",
    "        Dictionary where keys are model names and values are predicted labels.\n",
    "    label : str, optional\n",
    "        Description of the evaluation set (e.g., \"Validation Set\").\n",
    "    Returns:\n",
    "    pd.DataFrame containing accuracy, precision, recall, and F1 for each model.\n",
    "    \"\"\"\n",
    "\n",
    "    metrics = {\n",
    "        'Model': [],\n",
    "        'Accuracy': [],\n",
    "        'Precision': [],\n",
    "        'Recall': [],\n",
    "        'F1-score': []\n",
    "    }\n",
    "\n",
    "    for model_name, y_pred in y_pred_dict.items():\n",
    "        print(f\"\\n===== {model_name}: {label if label else ''} =====\")\n",
    "        print(\"Classification Report:\")\n",
    "        print(classification_report(y_true, y_pred))\n",
    "\n",
    "        metrics['Model'].append(model_name)\n",
    "        metrics['Accuracy'].append(accuracy_score(y_true, y_pred))\n",
    "\n",
    "        unique_labels = np.unique(y_true)\n",
    "        if len(unique_labels) == 2:\n",
    "            metrics['Precision'].append(precision_score(y_true, y_pred, average='binary', pos_label=unique_labels[1]))\n",
    "            metrics['Recall'].append(recall_score(y_true, y_pred, average='binary', pos_label=unique_labels[1]))\n",
    "            metrics['F1-score'].append(f1_score(y_true, y_pred, average='binary', pos_label=unique_labels[1]))\n",
    "        else:\n",
    "            metrics['Precision'].append(precision_score(y_true, y_pred, average='weighted'))\n",
    "            metrics['Recall'].append(recall_score(y_true, y_pred, average='weighted'))\n",
    "            metrics['F1-score'].append(f1_score(y_true, y_pred, average='weighted'))\n",
    "\n",
    "    return pd.DataFrame(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_pipeline_predict_evaluate(\n",
    "    model_class,\n",
    "    model_name,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    vectorizer_type='tfidf',\n",
    "    ngram_range=(1,1)\n",
    "):\n",
    "    if vectorizer_type == 'tfidf':\n",
    "        vectorizer = TfidfVectorizer(stop_words='english', ngram_range=ngram_range)\n",
    "    elif vectorizer_type == 'bow':\n",
    "        vectorizer = CountVectorizer(stop_words='english', ngram_range=ngram_range)\n",
    "    else:\n",
    "        raise ValueError(\"vectorizer_type must be either 'tfidf' or 'bow'\")\n",
    "\n",
    "    pipeline = make_pipeline(vectorizer, model_class)\n",
    "\n",
    "    print(f\"Fitting {model_name}...\")\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    print(f\"Predicting on training set...\")\n",
    "    y_pred_train = pipeline.predict(X_train)\n",
    "\n",
    "    print(f\"Predicting on validation set...\")\n",
    "    y_pred_val = pipeline.predict(X_val)\n",
    "\n",
    "    print(f\"Evaluating {model_name} on training set...\")\n",
    "    train_metrics = evaluate_models(y_train, {model_name: y_pred_train}, label=\"Train Set\")\n",
    "\n",
    "    print(f\"Evaluating {model_name} on validation set...\")\n",
    "    val_metrics = evaluate_models(y_val, {model_name: y_pred_val}, label=\"Validation Set\")\n",
    "\n",
    "    train_metrics[\"Dataset\"] = \"Train\"\n",
    "    val_metrics[\"Dataset\"] = \"Validation\"\n",
    "\n",
    "    return pd.concat([train_metrics, val_metrics], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag Of Words + Unigrams Vecotrizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Logistic Regression (BoW)...\n",
      "Predicting on training set...\n",
      "Predicting on validation set...\n",
      "Evaluating Logistic Regression (BoW) on training set...\n",
      "\n",
      "===== Logistic Regression (BoW): Train Set =====\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     12532\n",
      "           1       1.00      1.00      1.00     14578\n",
      "\n",
      "    accuracy                           1.00     27110\n",
      "   macro avg       1.00      1.00      1.00     27110\n",
      "weighted avg       1.00      1.00      1.00     27110\n",
      "\n",
      "Evaluating Logistic Regression (BoW) on validation set...\n",
      "\n",
      "===== Logistic Regression (BoW): Validation Set =====\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      2685\n",
      "           1       0.99      0.99      0.99      3124\n",
      "\n",
      "    accuracy                           0.99      5809\n",
      "   macro avg       0.99      0.99      0.99      5809\n",
      "weighted avg       0.99      0.99      0.99      5809\n",
      "\n",
      "                       Model  Accuracy  Precision    Recall  F1-score  \\\n",
      "0  Logistic Regression (BoW)  0.999852   0.999794  0.999931  0.999863   \n",
      "1  Logistic Regression (BoW)  0.993458   0.993602  0.994238  0.993920   \n",
      "\n",
      "      Dataset  \n",
      "0       Train  \n",
      "1  Validation  \n"
     ]
    }
   ],
   "source": [
    "logistic_bow_uni_metrics = fit_pipeline_predict_evaluate(\n",
    "    LogisticRegression(penalty='l2', C=0.1,max_iter=1000),\n",
    "    \"Logistic Regression (BoW)\",\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    vectorizer_type='bow',\n",
    "    ngram_range=(1, 1),\n",
    ")\n",
    "print(logistic_bow_uni_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Multinomial Naive Bayes (BoW Unigrams)...\n",
      "Predicting on training set...\n",
      "Predicting on validation set...\n",
      "Evaluating Multinomial Naive Bayes (BoW Unigrams) on training set...\n",
      "\n",
      "===== Multinomial Naive Bayes (BoW Unigrams): Train Set =====\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.96     12532\n",
      "           1       0.96      0.97      0.96     14578\n",
      "\n",
      "    accuracy                           0.96     27110\n",
      "   macro avg       0.96      0.96      0.96     27110\n",
      "weighted avg       0.96      0.96      0.96     27110\n",
      "\n",
      "Evaluating Multinomial Naive Bayes (BoW Unigrams) on validation set...\n",
      "\n",
      "===== Multinomial Naive Bayes (BoW Unigrams): Validation Set =====\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.94      0.94      2685\n",
      "           1       0.95      0.96      0.95      3124\n",
      "\n",
      "    accuracy                           0.95      5809\n",
      "   macro avg       0.95      0.95      0.95      5809\n",
      "weighted avg       0.95      0.95      0.95      5809\n",
      "\n",
      "                                    Model  Accuracy  Precision    Recall  \\\n",
      "0  Multinomial Naive Bayes (BoW Unigrams)  0.960531   0.956907  0.970298   \n",
      "1  Multinomial Naive Bayes (BoW Unigrams)  0.948700   0.946869  0.958387   \n",
      "\n",
      "   F1-score     Dataset  \n",
      "0  0.963556       Train  \n",
      "1  0.952593  Validation  \n"
     ]
    }
   ],
   "source": [
    "bow_uni_nb_metrics = fit_pipeline_predict_evaluate(\n",
    "    model_class=MultinomialNB(alpha=3),\n",
    "    model_name=\"Multinomial Naive Bayes (BoW Unigrams)\",\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    vectorizer_type='bow',\n",
    "    ngram_range=(1, 1)\n",
    ")\n",
    "\n",
    "print(bow_uni_nb_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Random Forest (BoW Unigrams)...\n",
      "Predicting on training set...\n",
      "Predicting on validation set...\n",
      "Evaluating Random Forest (BoW Unigrams) on training set...\n",
      "\n",
      "===== Random Forest (BoW Unigrams): Train Set =====\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98     12532\n",
      "           1       0.98      0.99      0.99     14578\n",
      "\n",
      "    accuracy                           0.98     27110\n",
      "   macro avg       0.99      0.98      0.98     27110\n",
      "weighted avg       0.98      0.98      0.98     27110\n",
      "\n",
      "Evaluating Random Forest (BoW Unigrams) on validation set...\n",
      "\n",
      "===== Random Forest (BoW Unigrams): Validation Set =====\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.96      2685\n",
      "           1       0.96      0.98      0.97      3124\n",
      "\n",
      "    accuracy                           0.97      5809\n",
      "   macro avg       0.97      0.97      0.97      5809\n",
      "weighted avg       0.97      0.97      0.97      5809\n",
      "\n",
      "                          Model  Accuracy  Precision    Recall  F1-score  \\\n",
      "0  Random Forest (BoW Unigrams)  0.984803   0.978387  0.993689  0.985979   \n",
      "1  Random Forest (BoW Unigrams)  0.967464   0.959887  0.980474  0.970071   \n",
      "\n",
      "      Dataset  \n",
      "0       Train  \n",
      "1  Validation  \n"
     ]
    }
   ],
   "source": [
    "bow_uni_rf_metrics = fit_pipeline_predict_evaluate(\n",
    "    model_class=RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=120,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=7),\n",
    "    model_name=\"Random Forest (BoW Unigrams)\",\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    vectorizer_type='bow',\n",
    "    ngram_range=(1, 1)\n",
    ")\n",
    "\n",
    "print(bow_uni_rf_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words + Bigrams Vecotrizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Logistic Regression (BoW + Bigrams)...\n",
      "Predicting on training set...\n",
      "Predicting on validation set...\n",
      "Evaluating Logistic Regression (BoW + Bigrams) on training set...\n",
      "\n",
      "===== Logistic Regression (BoW + Bigrams): Train Set =====\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     12532\n",
      "           1       1.00      1.00      1.00     14578\n",
      "\n",
      "    accuracy                           1.00     27110\n",
      "   macro avg       1.00      1.00      1.00     27110\n",
      "weighted avg       1.00      1.00      1.00     27110\n",
      "\n",
      "Evaluating Logistic Regression (BoW + Bigrams) on validation set...\n",
      "\n",
      "===== Logistic Regression (BoW + Bigrams): Validation Set =====\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      2685\n",
      "           1       0.99      0.99      0.99      3124\n",
      "\n",
      "    accuracy                           0.99      5809\n",
      "   macro avg       0.99      0.99      0.99      5809\n",
      "weighted avg       0.99      0.99      0.99      5809\n",
      "\n",
      "                                 Model  Accuracy  Precision    Recall  \\\n",
      "0  Logistic Regression (BoW + Bigrams)  0.999963   1.000000  0.999931   \n",
      "1  Logistic Regression (BoW + Bigrams)  0.993458   0.992971  0.994878   \n",
      "\n",
      "   F1-score     Dataset  \n",
      "0  0.999966       Train  \n",
      "1  0.993924  Validation  \n"
     ]
    }
   ],
   "source": [
    "logistic_bow_bigrams_metrics = fit_pipeline_predict_evaluate(\n",
    "    LogisticRegression(penalty='l2', C=0.05,max_iter=1000),\n",
    "    \"Logistic Regression (BoW + Bigrams)\",\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    vectorizer_type='bow',\n",
    "    ngram_range=(1, 2)\n",
    ")\n",
    "print(logistic_bow_bigrams_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Multinomial Naive Bayes (BoW Bigrams)...\n",
      "Predicting on training set...\n",
      "Predicting on validation set...\n",
      "Evaluating Multinomial Naive Bayes (BoW Bigrams) on training set...\n",
      "\n",
      "===== Multinomial Naive Bayes (BoW Bigrams): Train Set =====\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.93      0.96     12532\n",
      "           1       0.94      1.00      0.97     14578\n",
      "\n",
      "    accuracy                           0.96     27110\n",
      "   macro avg       0.97      0.96      0.96     27110\n",
      "weighted avg       0.97      0.96      0.96     27110\n",
      "\n",
      "Evaluating Multinomial Naive Bayes (BoW Bigrams) on validation set...\n",
      "\n",
      "===== Multinomial Naive Bayes (BoW Bigrams): Validation Set =====\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.88      0.93      2685\n",
      "           1       0.90      0.99      0.94      3124\n",
      "\n",
      "    accuracy                           0.94      5809\n",
      "   macro avg       0.94      0.93      0.94      5809\n",
      "weighted avg       0.94      0.94      0.94      5809\n",
      "\n",
      "                                   Model  Accuracy  Precision    Recall  \\\n",
      "0  Multinomial Naive Bayes (BoW Bigrams)  0.963519   0.939973  0.995747   \n",
      "1  Multinomial Naive Bayes (BoW Bigrams)  0.937339   0.902802  0.990077   \n",
      "\n",
      "   F1-score     Dataset  \n",
      "0  0.967056       Train  \n",
      "1  0.944427  Validation  \n"
     ]
    }
   ],
   "source": [
    "bow_bi_nb_metrics = fit_pipeline_predict_evaluate(\n",
    "    model_class=MultinomialNB(alpha=10),\n",
    "    model_name=\"Multinomial Naive Bayes (BoW Bigrams)\",\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    vectorizer_type='bow',\n",
    "    ngram_range=(1, 2)\n",
    ")\n",
    "\n",
    "print(bow_bi_nb_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Random Forest (BoW Bigrams)...\n",
      "Predicting on training set...\n",
      "Predicting on validation set...\n",
      "Evaluating Random Forest (BoW Bigrams) on training set...\n",
      "\n",
      "===== Random Forest (BoW Bigrams): Train Set =====\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.95      0.97     12532\n",
      "           1       0.96      0.99      0.98     14578\n",
      "\n",
      "    accuracy                           0.97     27110\n",
      "   macro avg       0.97      0.97      0.97     27110\n",
      "weighted avg       0.97      0.97      0.97     27110\n",
      "\n",
      "Evaluating Random Forest (BoW Bigrams) on validation set...\n",
      "\n",
      "===== Random Forest (BoW Bigrams): Validation Set =====\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96      2685\n",
      "           1       0.95      0.98      0.97      3124\n",
      "\n",
      "    accuracy                           0.96      5809\n",
      "   macro avg       0.97      0.96      0.96      5809\n",
      "weighted avg       0.96      0.96      0.96      5809\n",
      "\n",
      "                         Model  Accuracy  Precision    Recall  F1-score  \\\n",
      "0  Random Forest (BoW Bigrams)  0.972741   0.960778  0.989711  0.975030   \n",
      "1  Random Forest (BoW Bigrams)  0.963333   0.948813  0.984955  0.966546   \n",
      "\n",
      "      Dataset  \n",
      "0       Train  \n",
      "1  Validation  \n"
     ]
    }
   ],
   "source": [
    "bow_bi_rf_metrics = fit_pipeline_predict_evaluate(\n",
    "    model_class=RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=120,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=7),\n",
    "    model_name=\"Random Forest (BoW Bigrams)\",\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    vectorizer_type='bow',\n",
    "    ngram_range=(1, 2)\n",
    ")\n",
    "\n",
    "print(bow_bi_rf_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Logistic Regression...\n",
      "Predicting on training set...\n",
      "Predicting on validation set...\n",
      "Evaluating Logistic Regression on training set...\n",
      "\n",
      "===== Logistic Regression: Train Set =====\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97     12532\n",
      "           1       0.97      0.98      0.97     14578\n",
      "\n",
      "    accuracy                           0.97     27110\n",
      "   macro avg       0.97      0.97      0.97     27110\n",
      "weighted avg       0.97      0.97      0.97     27110\n",
      "\n",
      "Evaluating Logistic Regression on validation set...\n",
      "\n",
      "===== Logistic Regression: Validation Set =====\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.95      0.96      2685\n",
      "           1       0.96      0.98      0.97      3124\n",
      "\n",
      "    accuracy                           0.96      5809\n",
      "   macro avg       0.97      0.96      0.96      5809\n",
      "weighted avg       0.96      0.96      0.96      5809\n",
      "\n",
      "                 Model  Accuracy  Precision    Recall  F1-score     Dataset\n",
      "0  Logistic Regression  0.971487   0.965348  0.982234  0.973717       Train\n",
      "1  Logistic Regression  0.964538   0.959094  0.975672  0.967312  Validation\n"
     ]
    }
   ],
   "source": [
    "logistic_tfidf_metrics = fit_pipeline_predict_evaluate(\n",
    "    LogisticRegression(penalty='l2', C=0.1, max_iter=1000),\n",
    "    \"Logistic Regression\",\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    vectorizer_type='tfidf',\n",
    "    ngram_range=(1, 1)\n",
    ")\n",
    "print(logistic_tfidf_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Multinomial Naive Bayes (TF-IDF Unigrams)...\n",
      "Predicting on training set...\n",
      "Predicting on validation set...\n",
      "Evaluating Multinomial Naive Bayes (TF-IDF Unigrams) on training set...\n",
      "\n",
      "===== Multinomial Naive Bayes (TF-IDF Unigrams): Train Set =====\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.81      0.89     12532\n",
      "           1       0.86      0.99      0.92     14578\n",
      "\n",
      "    accuracy                           0.91     27110\n",
      "   macro avg       0.92      0.90      0.90     27110\n",
      "weighted avg       0.92      0.91      0.90     27110\n",
      "\n",
      "Evaluating Multinomial Naive Bayes (TF-IDF Unigrams) on validation set...\n",
      "\n",
      "===== Multinomial Naive Bayes (TF-IDF Unigrams): Validation Set =====\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.79      0.87      2685\n",
      "           1       0.85      0.98      0.91      3124\n",
      "\n",
      "    accuracy                           0.90      5809\n",
      "   macro avg       0.91      0.89      0.89      5809\n",
      "weighted avg       0.91      0.90      0.89      5809\n",
      "\n",
      "                                       Model  Accuracy  Precision    Recall  \\\n",
      "0  Multinomial Naive Bayes (TF-IDF Unigrams)  0.906160   0.858070  0.989093   \n",
      "1  Multinomial Naive Bayes (TF-IDF Unigrams)  0.895163   0.846133  0.983995   \n",
      "\n",
      "   F1-score     Dataset  \n",
      "0  0.918934       Train  \n",
      "1  0.909871  Validation  \n"
     ]
    }
   ],
   "source": [
    "tfidf_uni_nb_metrics = fit_pipeline_predict_evaluate(\n",
    "    model_class=MultinomialNB(alpha=7),\n",
    "    model_name=\"Multinomial Naive Bayes (TF-IDF Unigrams)\",\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    vectorizer_type='tfidf',\n",
    "    ngram_range=(1, 1)\n",
    ")\n",
    "\n",
    "print(tfidf_uni_nb_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Random Forest (TF-IDF Unigrams)...\n",
      "Predicting on training set...\n",
      "Predicting on validation set...\n",
      "Evaluating Random Forest (TF-IDF Unigrams) on training set...\n",
      "\n",
      "===== Random Forest (TF-IDF Unigrams): Train Set =====\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99     12532\n",
      "           1       0.99      1.00      0.99     14578\n",
      "\n",
      "    accuracy                           0.99     27110\n",
      "   macro avg       0.99      0.99      0.99     27110\n",
      "weighted avg       0.99      0.99      0.99     27110\n",
      "\n",
      "Evaluating Random Forest (TF-IDF Unigrams) on validation set...\n",
      "\n",
      "===== Random Forest (TF-IDF Unigrams): Validation Set =====\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97      2685\n",
      "           1       0.97      0.98      0.98      3124\n",
      "\n",
      "    accuracy                           0.97      5809\n",
      "   macro avg       0.97      0.97      0.97      5809\n",
      "weighted avg       0.97      0.97      0.97      5809\n",
      "\n",
      "                             Model  Accuracy  Precision    Recall  F1-score  \\\n",
      "0  Random Forest (TF-IDF Unigrams)  0.989782   0.985867  0.995267  0.990544   \n",
      "1  Random Forest (TF-IDF Unigrams)  0.973145   0.967254  0.983355  0.975238   \n",
      "\n",
      "      Dataset  \n",
      "0       Train  \n",
      "1  Validation  \n"
     ]
    }
   ],
   "source": [
    "tfidf_uni_rf_metrics = fit_pipeline_predict_evaluate(\n",
    "    model_class=RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=120,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=7),\n",
    "    model_name=\"Random Forest (TF-IDF Unigrams)\",\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    vectorizer_type='tfidf',\n",
    "    ngram_range=(1, 1)\n",
    ")\n",
    "\n",
    "print(tfidf_uni_rf_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
